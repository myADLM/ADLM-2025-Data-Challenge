import json

from tqdm import tqdm
from django.core.management.base import BaseCommand
from django.utils import timezone

from api.llm import QwenLLM, OllamaLLM
from api.models import Chunk
from graph.models import Node

NER_SYSTEM_PROMPT = """
You are an expert linguist that extracts named entities from text. You will 
be given a chunk of text and you will need to extract the named entities 
from the text. Be thorough and permissive in your extraction we want to cast
a wide net for entities we extract.

Return only the JSON list of named entities in the following format:
["Named Entity 1", "Named Entity 2", "Named Entity 3", ...]

Return only the valid json response. Do not include any other text or commentary.

Examples:

Text: ```To evaluate potential carry-over/cross-contamination of the SBPP a Carry-over Study 
was conducted. Briefly a contrived stool sample containing a high concentration of an analyte was 
alternated with a clinical negative stool sample in 10 consecutive testing rounds. The high positive 
sample was generated by adding previously frozen and quantified enriched broth cultures into pooled negative, 
preserved, clinical stool at a final concentration ≥ 1 x 106 CFU/mL. The organisms used in this 
study were C. coli (ATCC 43486), E. coli (ATCC 43895 stx1+/stx2+/O157+), S. bongori (ATCC 43975), and 
S. flexneri (ATCC 25929). The negative sample was stool from symptomatic patients that previously tested 
negative for all SBPP targets.```

["carry-over", "cross-contamination", "SBPP", "Carry-over Study", "stool sample", "analyte", 
"clinical negative stool sample", "frozen broth cultures", "quantified enriched broth cultures",
"≥ 1 x 106 CFU/mL", "organisms", "C. coli", "ATCC 43486", "E. coli", "ATCC 43895 stx1+/stx2+/O157+",
"S. bongori", "ATCC 43975", "S. flexneri", "ATCC 25929", "symptomatic patients", "SBPP targets"]

Text: ```The LOB (limit of the blank), the LOD (limit of detection) and the LOQ (limit of quantitation) were determined in accordance with CLSI EP 17-A guideline, Protocols for Determina
tion of Limits of Detection).```

["LOB", "limit of the blank", "LOD", "limit of detection", "LOQ", "limit of quantitation", "CLSI EP 17-A guideline", "Protocols for Determination of Limits of Detection"]
"""

def extract_ner_batch(texts: list[str], llm):
    """Extract named entities from a batch of texts."""
    prompts = [f"""
    Text: ```{text}```

    """ for text in texts]
    
    responses = list(llm.chat(
        prompts, 
        stream=False, 
        system_prompt=NER_SYSTEM_PROMPT
    ))
    print('-responses', len(responses))
    
    results = []
    for text, response in zip(texts, responses):
        try:
            response_text = response.content
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            print(f'---Response text: {response_text}')
            response_list = json.loads(response_text)

            entity_indices = []
            for entity in response_list:
                try:
                    entity_indices.append(text.index(entity))
                except ValueError:
                    # Entity not found in text, skip it
                    continue

            response_list_with_indices = list(zip(response_list, entity_indices))
            results.append(response_list_with_indices)
        except Exception as e:
            results.append((None, str(e)))  # Store error for this text
    
    return results

def process_batch(chunks: list, llm, extraction_model: str):
    """Process a batch of chunks."""
    texts = [chunk.text for chunk in chunks]
    chunk_ids = [chunk.id for chunk in chunks]
    
    ner_results = extract_ner_batch(texts, llm)
    
    for chunk_id, chunk_text, ner_result in zip(chunk_ids, texts, ner_results):
        chunk = Chunk.objects.get(id=chunk_id)
        
        if isinstance(ner_result, tuple) and ner_result[0] is None:
            # Error occurred during extraction
            print(f'Error parsing entities for chunk {chunk_id}: {ner_result}')
            chunk.entities_parse_failed_at = timezone.now()
            chunk.entities_parse_failed_message = ner_result[1]
            chunk.save()
            continue
        
        try:
            for entity, index in ner_result:
                Node.objects.create(
                    name=entity,
                    chunk=chunk,
                    chunk_index=index,
                    kind='entity',
                    extraction_model=extraction_model
                )
            print(f'Parsed {len(ner_result)} entities for chunk {chunk_id}, len: {len(chunk_text)}')
            chunk.entities_parsed_at = timezone.now()
            chunk.save()
        except Exception as e:
            print(f'Error parsing entities for chunk {chunk_id}: {e}')
            chunk.entities_parse_failed_at = timezone.now()
            chunk.entities_parse_failed_message = str(e)
            chunk.save()

class Command(BaseCommand):
    help = "Extract named entities from chunks"

    def add_arguments(self, parser):
        parser.add_argument('--device', type=str, default='cuda:3')
        parser.add_argument('--batch-size', type=int, default=4)
        parser.add_argument('--model', type=str, default='qwen')

    def handle(self, *args, **options):
        print("Extracting named entities from chunks")

        # query for the number of chunks with parsed entities, 
        # errored, and yet to parse
        num_parsed_entities = Chunk.objects.filter(entities_parsed_at__isnull=False).count()
        num_errored_entities = Chunk.objects.filter(entities_parse_failed_at__isnull=False).count()
        num_pending_entities = Chunk.objects.filter(entities_parsed_at__isnull=True, entities_parse_failed_at__isnull=True).count()

        print(f'Chunk status: {num_parsed_entities} parsed, {num_errored_entities} errored, {num_pending_entities} pending')

        # filter only to those we haven't parsed entities for yet
        queryset = Chunk.objects.filter(
            is_active=True,
            entities_parsed_at__isnull=True,
            entities_parse_failed_at__isnull=True
        ).order_by('?')[:10000]

        # for debugging
        #queryset = queryset[:10]

        batch_size = options['batch_size']
        device = options['device']
        
        from api.llm import OpenAILLM
        llm = OpenAILLM(model_name='openai/gpt-oss-20b')
        extraction_model = 'openai/gpt-oss-20b'
        
        # Process queryset in batches
        batch_chunks = []
        for chunk in tqdm(queryset, desc="Processing chunks"):
            batch_chunks.append(chunk)
            
            if len(batch_chunks) >= batch_size:
                process_batch(batch_chunks, llm, extraction_model)
                batch_chunks = []
        
        # Process remaining chunks if any
        if batch_chunks:
            process_batch(batch_chunks, llm, extraction_model)
