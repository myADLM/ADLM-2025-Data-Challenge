# 510(k) SUBSTANTIAL EQUIVALENCE DETERMINATION DECISION SUMMARY

I Background Information:

A 510(k) Number K233204   
B Applicant Philips Medical Systems Nederland B.V.   
C Proprietary and Established Names Philips IntelliSite Pathology Solution 5.1   
D Regulatory Information

<table><tr><td rowspan=1 colspan=1>ProductCode(s)</td><td rowspan=1 colspan=1>Classification</td><td rowspan=1 colspan=1>RegulationSection</td><td rowspan=1 colspan=1>Panel</td></tr><tr><td rowspan=1 colspan=1>PSY</td><td rowspan=1 colspan=1>Class Ⅱ</td><td rowspan=1 colspan=1>21 CFR 864.3700 -Whole Slide Imaging System</td><td rowspan=1 colspan=1>PA - Pathology</td></tr></table>

# II Submission/Device Overview:

A Purpose for Submission:

1. New device: Philips IntelliSite Pathology Solution 5.1 comprising the Pathology Scanner SG20, Pathology Scanner SG60, Pathology Scanner SG300, Image Management System 4.2 and PP27QHD Display.

2. Update of Image Management System 4.1 (IMS 4.1) to IMS 4.2 to support the previously cleared Ultra Fast Scanner (UFS) in K203854.

# B Type of Test:

Digital pathology whole slide imaging

III Intended Use/Indications for Use:

A Intended Use(s):

See Indications for Use below.

# B Indication(s) for Use:

The Philips IntelliSite Pathology Solution (PIPS) 5.1 is an automated digital slide creation, viewing, and management system. The PIPS 5.1 is intended for in vitro diagnostic use as an aid to the pathologist to review and interpret digital images of surgical pathology slides prepared from formalin-fixed paraffin embedded (FFPE) tissue. The PIPS 5.1 is not intended for use with frozen section, cytology, or non-FFPE hematopathology specimens.

The PIPS 5.1 comprises the Image Management System (IMS) 4.2, Ultra Fast Scanner (UFS), Pathology Scanner SG20, Pathology Scanner SG60, Pathology Scanner SG300 and PP27QHD Display. The PIPS 5.1 is for creation and viewing of digital images of scanned glass slides that would otherwise be appropriate for manual visualization by conventional light microscopy. It is the responsibility of a qualified pathologist to employ appropriate procedures and safeguards to assure the validity of the interpretation of images obtained using PIPS 5.1.

# C Special Conditions for Use Statement(s):

For in vitro diagnostic (IVD) use only

Rx - For Prescription Use Only

# IV Device/System Characteristics:

# A Device Description:

The Philips IntelliSite Pathology Solution 5.1 (PIPS 5.1), is an automated digital slide creation, viewing, and management system. PIPS 5.1 consists of two subsystems and a display component:

1. Subsystems:

a. Whole slide imaging scanners: i. Ultra Fast Scanner (UFS) ii. Pathology Scanner SG with different versions for varying slide capacity: Pathology Scanner SG20, Pathology Scanner SG60, Pathology Scanner SG300 b. Image Management System (IMS) 4.2 2. PP27QHD Display

The 3 pathology scanner SG models (versions) SG20, SG60 and SG 300 share the same imaging pipeline, which is implemented in the scan engine (Second Generation Scanner Engine-SGSE). Based on the System Design Specification documents, the differences between the 3 pathology scanner SG models are mainly the slide feeders and housing. The Pathology Scanner SG20 contains 1 slide rack with a maximum capacity of 20 slides, Pathology Scanner SG60 contains 3 slide racks with a maximum of 20 slides per rack for a total capacity of 60 slides and Pathology Scanner SG300 contains 15 slide racks with a maximum of 20 slides each per rack for a total capacity of 300 slides. The three models of the Pathology Scanner - SG20, SG60, SG300 are considered identical and interchangeable in terms of the imaging pipeline.

The slides are manually loaded in racks which are then placed in rack slots in the scanners. The pathology scanner SG20/SG60/SG300 consist of optical, mechanical, electronic and software components to scan FFPE tissue mounted on glass slides at a resolution equivalent to a 40x objective to create digital whole slide images (WSI). The pathology scanner SG20/SG60/SG300 take macro images (snapshot images of the entire glass slide) as well as the glass slide label and automatically decode the barcode, as applicable. Based on the macro image of the slide, the scanner determines which region(s) on the slide contain tissue and scans these region(s). All images including the WSI together with information about decoded barcode are sent to the IMS server. The scanning progress is shown on the touchscreen display of the scanner. Proprietary software is used for image processing during acquisition. Philips’ proprietary format, iSyntax, is used to store and transmit the images between the scanners and the IMS.

There are no changes to the UFS since the previous clearance in K203845.

The IMS 4.2 is a software only subsystem of PIPS 5.1 and consists of the IMS Review Application Software and IMS Application Server & Storage software installed on compatible server hardware. IMS Review Application Software is a HTML based web application and accessed via a web- browser on the user workstation. The IMS 4.2 organizes the WSIs into cases using a connection to the laboratory information system (LIS). The pathologist uses the IMS Review Application Software to view and manage WSIs and the corresponding cases. It allows the pathologist to pan, zoom, make measurements, create annotations and bookmark WSIs. IMS 4.2 was upgraded from the previous version cleared for use with UFS in K203854 to support both UFS and Pathology Scanner SG20, SG60 and SG300.

There are no changes to the PP27QHD display from the previous clearance K203845. The PP27QHD display allows the slide images to be viewed and is connected to a client computer with access to the IMS Review Application Software. The PP27QHD display is calibrated using the built-in calibration sensor and software tool, which automatically performs a periodic calibration.

# B Instrument Description Information:

1. Instrument Name: Philips IntelliSite Pathology Solution 5.1

2. Specimen Identification:

Glass slides and scanned images are identified based on the previously assigned specimen identifiers such as patient identifiers, barcodes, etc. Digital images of surgical pathology slides prepared from formalin-fixed paraffin embedded (FFPE) tissue.

# 3. Specimen Sampling and Handling:

Specimen sampling and handling are performed upstream and independent of the use of the subject device. Specimen sampling includes surgical pathology specimens such as biopsy or resection specimens which are processed using standard histology techniques. The FFPE tissue sections are H&E stained. Then digital images are obtained from these glass slides using the UFS or Pathology scanner SG20, SG60 or SG300.

4. Calibration:

The SG scanner performs a check on the image quality parameters for each scan. If the check fails, the SG scanner will automatically start to calibrate. When a calibration aspect that affects the image quality is outside the pre-specified tolerances, the user interface shows a “calibration failed” message, and the scanner will abort the scanning process. Then the user is prompted to perform calibration via the maintenance tab in the user interface. In addition to the automated calibration within the pathology scanner SG20/SG60/SG300, routine manual calibration is performed during field service visits.

The display is calibrated using a built-in front sensor. Calibrations are initiated by the QAWeb agent and performed as a background activity. The IMS subsystem does not require calibration.

# 5. Quality Control:

It is the responsibility of the laboratory staff to conduct and maintain quality control of the slides per their laboratory standards (e.g., staining, cover-slipping, barcode placement) prior to loading the slides into the pathology scanner SG20/SG60/SG300. After completing a scan, the operator checks the image data and quality using the IMS Viewer per instructions for use and the slide is rescanned, as needed. In addition to calibrations, quality checks for the display are initiated by the QAWeb agent and performed as a background activity.

# V Substantial Equivalence Information:

A Predicate Device Name(s): Philips IntelliSite Pathology Solution   
B Predicate 510(k) Number(s): K203845   
C Comparison with Predicate(s):

Table 1: Comparison of Technological Characteristics   

<table><tr><td colspan="1" rowspan="1">Device &amp; PredicateDevice(s):</td><td colspan="1" rowspan="1">K203845Philips Intellisite Pathology Solution Philips Intellisite Pathology Solution 5.1(PIPS)</td><td colspan="1" rowspan="1">K233204Philips Intellisite Pathology Solution Philips Intellisite Pathology Solution 5.1(PIPS 5.1)</td></tr><tr><td colspan="3" rowspan="1">General Device Characteristic: Similarities</td></tr><tr><td colspan="1" rowspan="1">Intended Use /Indications for Use</td><td colspan="1" rowspan="1">The Philips IntelliSite PathologySolution (PIPS) is an automateddigital slide creation, viewing, andmanagement system. The PIPS isintended for in vitro diagnostic use asan aid to the pathologist to review andaid to the pathologist to review andinterpret digital images of surgicalpathology slides prepared fromformalin-fixed paraffin embedded(FFPE) tissue. The PIPS is notintended for use with frozen section,</td><td colspan="1" rowspan="1">The Philips IntelliSite PathologySolution (PIPS) 5.1 is an automateddigital slide creation, viewing, andmanagement system. The PIPS 5.1 is intended for in vitro diagnostic use as anan aid to the pathologist to review andaid to the pathologist to review andinterpret digital images of surgicalpathology slides prepared fromformalin-fixed paraffin embedded(FFPE) tissue. The PIPS 5.1 is notintended for use with frozen section,</td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1">cytology, or non-FFPEhematopathology specimens.The PIPS comprises the ImageManagement System (IMS), the Ultra Management System (IMS) 4.2, theFast Scanner (UFS), and Display. TheUltra Fast Scanner (UFS), PathologyPIPS is for creation and viewing ofdigital images of scanned glass slidesthat would otherwise be appropriatefor manual visualization byconventional light microscopy. It isthe responsibility of a qualifiedpathologist to employ appropriateprocedures and safeguards to assurethe validity of the interpretation ofimages obtained using PIPS.</td><td colspan="1" rowspan="1">cytology, or non-FFPE hematopathologyspecimens.The PIPS 5.1 comprises the ImageManagement System (IMS), the Ultra Management System (IMS) 4.2, theFast Scanner (UFS), and Display. TheUltra Fast Scanner (UFS), PathologyScanner SG20, Pathology ScannerSG60, Pathology Scanner SG300 andPP27QHD Display. The PIPS 5.1 is forcreation and viewing of digital images ofscanned glass slides that wouldotherwise be appropriate for manualvisualization by conventional lightmicroscopy. It is the responsibility of aqualified pathologist to employappropriate procedures and safeguards toassure the validity of the interpretationof images obtained using PIPS 5.1.</td></tr><tr><td colspan="1" rowspan="1">Principle ofOperation</td><td colspan="1" rowspan="1">The technician loads the slides intothe WSI scanner. The scanner scansthe slides and generates WSI imagefor each slide. The technicianperforms Quality Control (QC) onscanned WSI images and rescan theslide when QC is failed. The acquiredWSI images are stored in an end userprovided image storage attached tothe local network.During review, the pathologist opensWSI images acquired with the WSIscanner from the image storage,performs further QC and reads WSIimages of the slides to make adiagnosis.</td><td colspan="1" rowspan="1">Same</td></tr><tr><td colspan="1" rowspan="1">DeviceComponents</td><td colspan="1" rowspan="1">UFS, IMS and PP27QHD display.</td><td colspan="1" rowspan="1">UFS, Pathology Scanner SG20/SG60and SG300, IMS 4.2 and PP27QHDdisplay.</td></tr><tr><td colspan="1" rowspan="1">Image File Format</td><td colspan="1" rowspan="1">iSyntax</td><td colspan="1" rowspan="1">Same</td></tr><tr><td colspan="1" rowspan="1">End User'sInterface</td><td colspan="1" rowspan="1">The IMS Image Processing Pipeline4.1 is designed to process UFSiSyntax data.</td><td colspan="1" rowspan="1">The IMS Image Processing Pipeline 4.2is designed to process both UFS andPathology Scanner SG20/SG60 andSG300 iSyntax data.</td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="2" rowspan="1">General Device Characteristic: Differences</td></tr><tr><td colspan="1" rowspan="1">Slide Feeder</td><td colspan="1" rowspan="1">Storage with a capacity of 15racks (300 slides).</td><td colspan="1" rowspan="1">Three slide storage capacities:SG 300: 15 racks (300 slides)SG 60:3 racks (60 slides)SG 20:1 rack (20 slides).</td></tr><tr><td>Imaging Optics</td><td>Three RGB high resolution cameras and one single monochrome focus camera (4 Digital Image Sensors in total).</td><td>Single camera with a single Digital Image Sensor.</td></tr></table>

# VI Standards/Guidance Documents Referenced:

1. Technical Performance Assessment of Digital Pathology Whole Slide Imaging Devices. Guidance for Industry and Food and Drug Administration Staff. April 20, 2016.   
2. Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices. Guidance for Industry and Food and Drug Administration Staff. June 14, 2023.   
3. Cybersecurity in Medical Devices: Quality System Considerations and Content of Premarket Submissions. Guidance for Industry and Food and Drug Administration Staff. September 27,   
2023.   
4. Off-The-Shelf Software Use in Medical Devices. Guidance for Industry and Food and Drug Administration Staff. August 11, 2023.   
5. IEC 62304 Medical device software – Software life cycle processes (Edition 1.1, 2015-06).

# VII Performance Characteristics:

# A. Analytical Performance:

# 1. Precision/Reproducibility:

The objective of this study was to evaluate the repeatability (within- and between- system) and reproducibility (between-site) of PIPS 5.1. The precision of the device was based on three reading pathologists’ assessment and identification of 21 specific clinically relevant histopathologic “features” that are observed in FFPE hematoxylin and eosin (H&E) stained slides. The slides with the selected study features were scanned using 3 magnifications: 10x, $2 0 \mathrm { x }$ and $4 0 \mathrm { x }$ . As shown in the Table 2 below, there were seven different study features per magnification. In total, 21 features were selected, with each feature selected from three different organs, to ensure that multiple tissue types were evaluated. For the reading part of the study, three non-study-features per magnification were added to the reading checklists as “wild card” slides to reduce recall bias but were excluded from the statistical analyses.

Table 2: Histologic Study Features   

<table><tr><td rowspan=9 colspan=1>Study Features</td><td rowspan=1 colspan=3>Magnification</td></tr><tr><td rowspan=1 colspan=1>40x</td><td rowspan=1 colspan=1>20x</td><td rowspan=1 colspan=1>10x</td></tr><tr><td rowspan=1 colspan=1>Nucleolus</td><td rowspan=1 colspan=1>Reed Sternberg</td><td rowspan=1 colspan=1>Small artery</td></tr><tr><td rowspan=1 colspan=1>Eosinophil granules</td><td rowspan=1 colspan=1>Polymorph neutrophil</td><td rowspan=1 colspan=1>Psammoma body</td></tr><tr><td rowspan=1 colspan=1>Mitosis</td><td rowspan=1 colspan=1>Plasma cell</td><td rowspan=1 colspan=1>Keratin pearl</td></tr><tr><td rowspan=1 colspan=1>Nuclear membrane</td><td rowspan=1 colspan=1>Goblet cell</td><td rowspan=1 colspan=1>Granuloma</td></tr><tr><td rowspan=1 colspan=1>Cilia</td><td rowspan=1 colspan=1>Tingible body macrophage</td><td rowspan=1 colspan=1>Adipose Cell</td></tr><tr><td rowspan=1 colspan=1>Prickles desmosomes</td><td rowspan=1 colspan=1>Foreign Body giant cell</td><td rowspan=1 colspan=1>Glandular formation</td></tr><tr><td rowspan=1 colspan=1>Pigment laden macrophages</td><td rowspan=1 colspan=1>Lymphocyte</td><td rowspan=1 colspan=1>Necrosis</td></tr><tr><td rowspan=3 colspan=1>Non-study-Features</td><td rowspan=1 colspan=1>Nuclear grooves</td><td rowspan=1 colspan=1>Mast cells</td><td rowspan=1 colspan=1>Peripheral nerves</td></tr><tr><td rowspan=1 colspan=1>Asteroid bodies</td><td rowspan=1 colspan=1>Chondrocytes</td><td rowspan=1 colspan=1>Germinal center</td></tr><tr><td rowspan=1 colspan=1>Nuclear inclusions</td><td rowspan=1 colspan=1>Apocrine cells</td><td rowspan=1 colspan=1>Secretions</td></tr></table>

The study slides were selected from previously accessioned cases from the laboratory archives. All slides were specimens from subjects who already had received their diagnosis. Slides containing the pre-specified study features were selected from consecutive cases using the laboratory information system (LIS) by an enrollment pathologist (EP) and confirmed by a validating enrollment pathologist (VEP) and scanned by a study technician. The EP reviewed the WSI and defined an area (bookmark) containing the selected feature(s) at the appropriate magnification. The VEP confirmed the presence of the feature(s) in the FOV before enrolling the image in the study. Three independent study reading pathologists reviewed the FOV as a snapshot with no panning or zooming capabilities. The reading pathologists assessed the presence of each of the features based on a feature checklist appropriate for that particular magnification.

One slide set $\scriptstyle ( \mathrm { n } = 3 9 9 )$ ) was used for all three sub-studies. Reading pathologists were different for each sub-study and were blinded to the study features. From a study slide set, 399 FOVs were extracted containing 420 selected features. In addition, a total of 210 wildcard FOVs was selected. The results from the FOV wildcards were not analyzed. The number of FOV wildcards added per reading session was 70 (approximately $1 7 . 5 \%$ of the total number of FOVs in the study). The acceptance criterion for precision was the lower-limit of the $9 5 \%$ confidence interval for the overall agreement rate is $8 5 . 0 \%$ or above.

Precision was assessed in three sub-studies: within-system, between-system and between-site.

• In the within-system study, precision within a system was evaluated on 3 systems by 3 pathologists at 1 site. In the between-system study, precision between systems was evaluated on 3 systems by 3 pathologists at 1 site. In the between-site study, precision between systems located at different sites was evaluated at 3 sites on 3 systems by 3 pathologists.

# Within-System Precision:

The study slide set was equally $[ \mathrm { n } { = } 1 3 3$ slides per system) and randomly divided over 3 systems at 1 site. On each system the slides were scanned 3 times with at least 6 hours of system downtime (ensuring full cool down) between scanning iterations. Three separate reading sessions were performed by each of 3 reading pathologists, with a washout period of at least 2 weeks between sessions. All FOVs from all systems were read by all pathologists. All 3 iterations of all 3 systems were read by each pathologist in a randomized way. Within-system precision results (overall and by system) are presented in the Table 3 below.

Table 3: Within-System Agreement Rates   

<table><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=2>Agreement rate</td></tr><tr><td rowspan=1 colspan=1>System</td><td rowspan=1 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=1>Number ofPairwise Agreements</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Overall</td><td rowspan=1 colspan=1>3600</td><td rowspan=1 colspan=1>3178</td><td rowspan=1 colspan=1>88.3</td><td rowspan=1 colspan=1>(86.7; 89.9)</td></tr><tr><td rowspan=1 colspan=1>Scanner1</td><td rowspan=1 colspan=1>1210</td><td rowspan=1 colspan=1>1054</td><td rowspan=1 colspan=1>87.1</td><td rowspan=1 colspan=1>(84.1; 90.0)</td></tr><tr><td rowspan=1 colspan=1>Scanner 2</td><td rowspan=1 colspan=1>1190</td><td rowspan=1 colspan=1>1049</td><td rowspan=1 colspan=1>88.2</td><td rowspan=1 colspan=1>(85.6; 90.7)</td></tr><tr><td rowspan=1 colspan=1>Scanner 3</td><td rowspan=1 colspan=1>1200</td><td rowspan=1 colspan=1>1075</td><td rowspan=1 colspan=1>89.6</td><td rowspan=1 colspan=1>(86.6; 92.4)</td></tr></table>

Between-System Precision: The complete study slide set $\scriptstyle ( \mathrm { n } = 3 9 9 $ ) was scanned once on each of the 3 systems at one site.

The same 210 wildcard FOVs were used from the intra-system study. Three separate reading sessions were performed by each pathologist with a washout period of at least 2 weeks between sessions in a randomized way. All FOVs from all systems were read by all pathologists. Between-system precision results (overall and by system pair) are presented in the Table below.

Table 4: Between-System Agreement Rates   

<table><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>cYanmaGllow</td><td rowspan=1 colspan=2>Agreement rate</td></tr><tr><td rowspan=1 colspan=1>System</td><td rowspan=1 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=1>Number ofPairwise Agreements</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Overall</td><td rowspan=1 colspan=1>3610</td><td rowspan=1 colspan=1>3445</td><td rowspan=1 colspan=1>95.4</td><td rowspan=1 colspan=1>(94.4; 96.5)</td></tr><tr><td rowspan=1 colspan=1>Scanner 1 vs. Scanner 2</td><td rowspan=1 colspan=1>1200</td><td rowspan=1 colspan=1>1144</td><td rowspan=1 colspan=1>95.3</td><td rowspan=1 colspan=1>(94.0; 96.6)</td></tr><tr><td rowspan=1 colspan=1>Scanner 1 vs. Scanner 3</td><td rowspan=1 colspan=1>1207</td><td rowspan=1 colspan=1>1152</td><td rowspan=1 colspan=1>95.4</td><td rowspan=1 colspan=1>(94.1; 96.7)</td></tr><tr><td rowspan=1 colspan=1>Scanner 2 vs. Scanner 3</td><td rowspan=1 colspan=1>1203</td><td rowspan=1 colspan=1>1149</td><td rowspan=1 colspan=1>95.5</td><td rowspan=1 colspan=1>(94.2; 96.7)</td></tr></table>

# Between-Site Reproducibility Study:

A different system and a different study technician were involved per site. The same slide set $\scriptstyle ( 1 = 3 9 9 )$ ) was scanned once at each site, resulting in 3 WSI sets. In the within-site study, there were 3 readings R1, R2 and R3 from 3 pathologists and 3 different systems, each at a different site, for each selected feature on an FOV. There were 3 pairwise comparisons among the 3 readings, i.e., R1-R2, R1-R3 and R2-R3. A pair of readings was considered in agreement when both readings were ‘present’ or when both readings were ’absent’. The primary endpoint, i.e., the overall within-site agreement rate (i.e., when the selected feature was marked as “present” in both readings or not marked in both readings), was calculated using all available pairwise comparison results over all enrolled features. Between-site reproducibility results (overall and by system pair) are presented in the Table below.

Table 5: Between-Site Agreement Rates   

<table><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=2>Agreement rate</td></tr><tr><td rowspan=1 colspan=1>Sites</td><td rowspan=1 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=1>Number ofPairwise Agreements</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Overall</td><td rowspan=1 colspan=1>1228</td><td rowspan=1 colspan=1>1114</td><td rowspan=1 colspan=1>90.7</td><td rowspan=1 colspan=1>(88.4; 92.9)</td></tr><tr><td rowspan=1 colspan=1>Site 1 vs Site 2</td><td rowspan=1 colspan=1>407</td><td rowspan=1 colspan=1>368</td><td rowspan=1 colspan=1>90.4</td><td rowspan=1 colspan=1>(87.5; 93.1)</td></tr><tr><td rowspan=1 colspan=1>Site 1 vs Site 3</td><td rowspan=1 colspan=1>406</td><td rowspan=1 colspan=1>371</td><td rowspan=1 colspan=1>91.4</td><td rowspan=1 colspan=1>(88.5; 94.1)</td></tr><tr><td rowspan=1 colspan=1>Site 2 vs Site 3</td><td rowspan=1 colspan=1>415</td><td rowspan=1 colspan=1>375</td><td rowspan=1 colspan=1>90.4</td><td rowspan=1 colspan=1>(87.5; 93.1)</td></tr></table>

2. Linearity:

Not applicable

3. Analytical Specificity/Interference: Not applicable

4. Accuracy (Instrument): Not applicable

5. Carry-Over: Not applicable

B. Technical Studies:

Studies were conducted to evaluate the performance of the new Pathology Scanner SG20/SG60/SG300 and IMS 4.2, as recommended in FDA guidance titled “Technical Performance Assessment of Digital Pathology Whole Slide Imaging Devices” as shown in the Table below.   

<table><tr><td colspan="1" rowspan="1">TPA item</td><td colspan="1" rowspan="1">Testing Data Description</td></tr><tr><td colspan="1" rowspan="1">Slide Feeder</td><td colspan="1" rowspan="1">Information was provided on the configurations of the slide feed mechanism,including a physical description of the slide, the number of slides in carriers,and the class of automation. Information was provided on the user interactionwith the slide feeder, including hardware, software, feedback mechanisms,and Failure Mode and Effects Analysis (FMEA).</td></tr><tr><td colspan="1" rowspan="1">Light Source</td><td colspan="1" rowspan="1">Descriptive information associated with the light source was provided.Technical information was provided to verify the spectral distribution of thelight that is incident on the slide to the slide is to verify that the colorreproducibility of the Pathology Scanner SG20/SG60/SG300 is withinpredefined limits. The tests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">Imaging optics</td><td colspan="1" rowspan="1">An optical schematic with all optical elements identified from slide (objectplane) to image sensor (micro camera) was provided. Descriptive informationregarding the microscope objective, auxiliary lens(es), and the magnificationof imaging optics was provided. Testing information regarding themagnification, relative irradiance, optical distortions, and chromaticaberrations was provided. The tests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">MechanicalscannerMovement</td><td colspan="1" rowspan="1">Information and specifications of the configuration of the stage, method ofmovement, control of movement of the stage, and FMEA was provided. Testdata to determine positioning accuracy and repeatability for the X-Y and Zstages was provided. The tests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">DigitalImaging sensor</td><td colspan="1" rowspan="1">Information and specifications on the sensor type, pixel information,responsivity specifications, noise specifications, readout rate, and digitaloutput format were provided. Testing to determine the correct functioning ofthe digital image sensor was provided. The tests passed their acceptancecriteria.</td></tr><tr><td colspan="1" rowspan="1">ImageProcessingSoftware</td><td colspan="1" rowspan="1">Information and specifications on the exposure control, white balance, colorcorrection, sub-sampling, pixel-offset correction, shading (flat-field)correction, and pixel-defect correction were provided.</td></tr><tr><td colspan="1" rowspan="1">Imagecomposition</td><td colspan="1" rowspan="1">Information and specifications on the scanning method and focus, wasprovided. Test data to analyze the image composition performance wasprovided. The tests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">Image fileformats</td><td colspan="1" rowspan="1">Information and specifications on the compression method, and compressionratio were provided.</td></tr><tr><td colspan="1" rowspan="1">Image ReviewManipulationSoftware</td><td colspan="1" rowspan="1">Information and specifications for continuous panning, continuous zooming,discrete Z-axis displacement, comparison of slides in multiple windows,annotation tools, image enhancement, color correction, tracking of visitedareas, digital bookmarks, and shared viewing sessions was provided. Testdata to analyze the image review manipulation software was provided. Thetests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">Display</td><td colspan="1" rowspan="1">No changes have been introduced for the PP27QHD display. Tests results forthis display in previous 510(k) remain valid.</td></tr><tr><td colspan="1" rowspan="1">ComputerEnvironment</td><td colspan="1" rowspan="1">Information and specifications on the computer hardware, operating system,memory, hard disk, graphics card, graphics card driver, color managementsettings, color profile, display interface and network were provided.</td></tr><tr><td colspan="1" rowspan="1">ColorReproducibility</td><td colspan="1" rowspan="1">Test data to quantify the accuracy and precision of the color transformationfrom the slide to the display monitor was provided. The tests passed theiracceptance criteria</td></tr><tr><td colspan="1" rowspan="1">SpatialResolution</td><td colspan="1" rowspan="1">Test data to evaluate the spatial resolution of the image acquisition phase wasprovided. The tests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">Focusing Test</td><td colspan="1" rowspan="1">Test data to demonstrate that the focus quality is clinically acceptable for avariety of histologic preparations, including different tissue types, stainintensities, specimen thicknesses, and stain types was provided. The testspassed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">Whole SlideTissueCoverage</td><td colspan="1" rowspan="1">Test data to demonstrate that the entire tissue specimen on the clinical slide isdetected by device was provided. The tests passed their acceptance criteria.</td></tr><tr><td colspan="1" rowspan="1">Stitching Error</td><td colspan="1" rowspan="1">Test data to evaluate the stitching quality of the high-resolution image of thePathology Scanner SG20/SG60/SG300. The tests passed their acceptancecriteria.</td></tr><tr><td colspan="1" rowspan="1">TurnaroundTime</td><td colspan="1" rowspan="1">Test data to evaluate the average time required to execute zooming andpanning operations, and to refresh the display in response to user input wasprovided. The tests passed their acceptance criteria.</td></tr></table>

# C. Clinical Performance:

A clinical validation study was conducted to demonstrate that viewing, reviewing and diagnosing surgical pathology tissue slides by using PIPS 5.1, manual digital (MD) is noninferior to using manual optical (light) microscope (MO) modality. The device configuration used in this study was Pathology Scanner SG300, IMS 4.2 and PP27QHD display. The primary endpoint was the paired MD-MO difference in the major discordance rates, derived from readings of the same cases using both MO and MD by the same reader. The MD modality is declared non-inferior to the MO modality if the upper bound of the $9 5 \%$ two-sided confidence interval for the overall MD – MO difference in major discordance rate (compared to the main diagnosis) was less than $4 \%$ . The study was conducted at 3 sites. Eleven reading pathologists participated in the study (4 reading pathologists at two sites, and 3 reading pathologists the third site). There was no subject enrollment or subject participation in this study and involved a review of previously accessioned cases from the laboratory archives. All slides were from subjects who had already received their diagnosis. The study consisted of 2 sets of cases:

• Set 1: This study set consisted of a subset of cases from the previous clinical study that was part of DEN160056. This clinical study was based on the reading of surgical pathology slides which included hematoxylin & eosin (H&E) stained slides, obtained from consecutive cases at least one year old and for which a sign-out diagnosis (final diagnosis assigned to the case at the local institution using a light microscope) was available. The main diagnosis and clinical information from the original study was imported and used in this study. Cases and corresponding original representative slides were selected from the clinical study enrollment set randomly per organ/diagnosis subtype.

Set 2: This set consisted of new cases, which were used in 2 of the 4 sites participating in the clinical study. As in the DEN160056 clinical study, each site had a site-specific list of organs to include and set 2 was added to ensure a total set that adequately represented routine clinical practice.   
Both sets were read with both the MO modality and the MD modality. An adjudication panel independently reviewed and compared the reader diagnosis against the main diagnosis to determine whether the diagnoses were concordant, minor discordant or major discordant according to pre-specified criteria. The study inclusion and exclusion criteria were as follows.

# Inclusion Criteria:

All H&E, IHC and special stains glass cover-slipped slide or slides, with human tissue obtained via surgical pathology of original case are available. Original sign-out diagnosis is available. Relevant clinical information that was available to the sign-out pathologist in the pathology request form is available. • Cases are at least one year since accessioning.

Both Set 1 and Set 2 had similar exclusion criteria which are as follows:

# Exclusion Criteria:

Cases, including sent out cases, for which any H&E, IHC or special stains slide used for the original sign-out diagnosis is no longer available at the site.   
Cases for which the control slides for IHC and special stains are not available. The selected slide or slides for the main diagnosis do not match any subtype of the organ for which the case was selected.   
Relevant clinical information that was available to the sign-out pathologist in the pathology request form cannot be obtained.   
The selected slide or slides for the main diagnosis and the control slide(s) do not fulfill the quality checks according to general clinical practice.   
Selected slides contain indelible markings.   
Selected slides with damaged tissue.   
More than one case was selected for a patient (only one case may be enrolled per patient).   
Case consists of frozen section(s) only.   
Case consists of gross specimens only.

The study consisted of five phases:

Study preparation: Each case was randomized and assigned different case IDs for the Manual Optical Read and the Manual Digital Read. • Enrollment: The enrollment pathologist performed a check of the glass slides against the exclusion criteria. Manual Optical Read: All slides were randomized by the study coordinator. All reading pathologists read all cases from their own site. Using a light microscope, the reading pathologists reviewed each case, and documented the main diagnosis obtained from the (representative) slide or slides in the electronic data capturing (system) (EDC).

There was a washout period of four weeks between the manual optical reading and the manual digital reading sessions.

Manual Digital Read: The study technician scanned all the randomized slides and upon completion of scanning, performed quality control (QC) on all images, in accordance with the QC procedures. All reading pathologists read all cases from their own site. The reading pathologists were provided with representative WSI(s) of the case at once without any possible bookmarks / annotations from other reading pathologists. The reading pathologist documented the main diagnosis obtained from the slide or slides in the EDC.

Adjudication of Study Diagnoses: Case by case adjudication was conducted for each case to determine whether the diagnosis was concordant, minor discordant or major discordant. A major discordance was defined as a difference in diagnosis that would be associated with a clinically important difference in patient management. A minor discordance was defined as a difference in diagnosis that would not be associated with a clinically important difference in patient management. Two adjudication pathologists (from the panel of three) independently reviewed and compared the reading pathologist’s diagnosis against the main diagnosis. The adjudication pathologist only reviewed the diagnoses and determined if the comparison was concordant, minor discordant or major discordant. If the concordance reviews from the two adjudicators were different, the third adjudication pathologist was assigned to independently perform a $3 ^ { \mathrm { r d } }$ concordance review. The ultimate score was determined by a majority vote. If all three adjudication pathologists disagreed after their initial review, the ultimate score for analysis was determined in an adjudication panel meeting where all three pathologists together reviewed only the two diagnoses and came to a final adjudication score by majority vote.

There were a total of 1735 slides from 952 cases [Site 1: 314 cases, 635 slides; Site 2: 345 cases, 627 slides; Site 3: 293 cases, 473 slides]. At each site, pathologists read all the cases assigned to the site using both the MO and the MD modalities with a washout period of four weeks in between, resulting in a total of 6988 readings.

Table 6: Clinical Study Results Based on Major Discordance Rates   

<table><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3>Manual Digital (MD)Discordance Rate</td><td rowspan=1 colspan=3>Manual Optical (MO)Discordance Rate</td><td rowspan=1 colspan=2>Difference MD - MO</td></tr><tr><td rowspan=1 colspan=1>Site</td><td rowspan=1 colspan=1>TotalReads</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td><td rowspan=1 colspan=1>TotalReads</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Observed</td><td rowspan=1 colspan=1>3494</td><td rowspan=1 colspan=1>5.8%</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>3494</td><td rowspan=1 colspan=1>5.7%</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>0.1%</td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=1>Model</td><td rowspan=1 colspan=1>3494</td><td rowspan=1 colspan=1>5.8%</td><td rowspan=1 colspan=1>(3.93; 8.53)</td><td rowspan=1 colspan=1>3494</td><td rowspan=1 colspan=1>5.7%</td><td rowspan=1 colspan=1>(3.87; 8.41)</td><td rowspan=1 colspan=1>0.1%</td><td rowspan=1 colspan=1>(-1.01; 1.18)</td></tr></table>

As shown in Table 6 above, the overall difference in major discordance rate (compared to the main diagnosis) for digital-optical was $0 . 1 \%$ , with a derived two-sided $9 5 \%$ CI of $[ - 1 . 0 1 \%$ ; $1 . 1 8 \% ]$ . As the upper limit of this confidence interval was less than the pre-specified noninferiority margin of $4 \%$ , manual digital diagnosing by a pathologist using PIPS 5.1 is noninferior to diagnosing by a pathologist using the optical microscope.

The differences in major discordance rates by organ types for the full analysis set between MD and MO are shown in the Table 7 below. The clinical study was not powered to analyze the results by individual organ site or diagnosis.

Table 7: Major Discordance Rate by Organ and Modality   

<table><tr><td rowspan=1 colspan=1>Organ</td><td rowspan=1 colspan=1>Manual Digital (MD)Manual Optical (MO)</td><td rowspan=1 colspan=1>Manual Digital (MD)Manual Optical (MO)</td><td rowspan=1 colspan=1>Difference (MD-MO)</td></tr><tr><td rowspan=1 colspan=1>Bladder</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>-6%</td></tr><tr><td rowspan=1 colspan=1>Brain/Neuro</td><td rowspan=1 colspan=1>26%</td><td rowspan=1 colspan=1>20%</td><td rowspan=1 colspan=1>6%</td></tr><tr><td rowspan=1 colspan=1>Breast</td><td rowspan=1 colspan=1>6%</td><td rowspan=1 colspan=1>4%</td><td rowspan=1 colspan=1>2%</td></tr><tr><td rowspan=1 colspan=1>Colorectal</td><td rowspan=1 colspan=1>6%</td><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>1%</td></tr><tr><td rowspan=1 colspan=1>Endocrine</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>12%</td><td rowspan=1 colspan=1>-2%</td></tr><tr><td rowspan=1 colspan=1>Gynecology</td><td rowspan=1 colspan=1>6%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>-1%</td></tr><tr><td rowspan=1 colspan=1>Kidney</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>0%</td></tr><tr><td rowspan=1 colspan=1>Liver/BD, Neo</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>11%</td><td rowspan=1 colspan=1>-4%</td></tr><tr><td rowspan=1 colspan=1>Lung</td><td rowspan=1 colspan=1>4%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>-3%</td></tr><tr><td rowspan=1 colspan=1>Lymph Node</td><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>0%</td></tr><tr><td rowspan=1 colspan=1>Prostate</td><td rowspan=1 colspan=1>18%</td><td rowspan=1 colspan=1>17%</td><td rowspan=1 colspan=1>1%</td></tr><tr><td rowspan=1 colspan=1>Salivary</td><td rowspan=1 colspan=1>14%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>7%</td></tr><tr><td rowspan=1 colspan=1>Skin</td><td rowspan=1 colspan=1>12%</td><td rowspan=1 colspan=1>12%</td><td rowspan=1 colspan=1>0%</td></tr><tr><td rowspan=1 colspan=1>Stomach</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>4%</td><td rowspan=1 colspan=1>-4%</td></tr></table>

# D. Other Supportive Instrument Performance Characteristics Data:

The use of IMS component (IMS 4.2) of the PIPS 5.1 with the previously cleared UFS was supported by bench testing data using the pixelwise comparison approach. A pixelwise comparison study was performed to compare images reproduced by the predicate device (PIPS 4.1) and the subject device (PIPS 5.1) for the same iSyntax files to demonstrate identical image reproduction. Devices used in this study included:

• Comparator (Predicate) device PIPS 4.1 consisting of UFS 1.8, IMS 4.1 and PP27QHD   
Display   
• Test device consisting of IMS 4.2 component of PIPS 5.1, UFS 1.8 and PP27QHD   
Display

A total of 42 glass slides from a diverse set of stain types and human anatomical sites were used as the test data. Three representative features from each of the 42 slides were selected and marked with a bookmark by a pathologist, resulting in a total of 252 regions of interest (ROIs) (42 slides x 3 ROIs x 2 magnification levels). Each location was captured at $2 0 \mathrm { x }$ and $4 0 \mathrm { x }$ magnification levels on both the predicate and subject devices. The client workstation and the PP27QHD display were used to capture ROIs and to perform the pixelwise comparison.

The pixelwise comparison test compared the screenshots of all the ROIs captured on the predicate device to those captured on the subject device, checking for an exact match in the RGB values of each pixel. The test results showed that the differences were zero for all pixels in all ROIs, indicating that all corresponding pixels are identical. These results demonstrated that the subject device PIPS 5.1 and the predicate device PIPS 4.1 generate identical images for the same iSyntax files generated by the UFS device.

# F. Human Factor Study:

This study was performed to obtain objective evidence that the user interface can be used safely and to assess the effectiveness of the usability-related risk management measures. Human factors studies were designed around user tasks, and use scenarios performed by the intended users. No user errors were identified that would result in serious harm to the patient or user. Overall, the results of the human factors testing were acceptable.

# VIII Proposed Labeling:

The labeling supports the finding of substantial equivalence for this device.

# IX Conclusion:

The submitted information in this premarket notification is complete and supports a substantial equivalence decision.