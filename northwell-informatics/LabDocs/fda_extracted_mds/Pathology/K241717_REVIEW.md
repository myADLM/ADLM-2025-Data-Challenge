# 510(k) SUBSTANTIAL EQUIVALENCE DETERMINATION DECISION SUMMARY

I Background Information:

# B Applicant

Shandon Diagnostics Limited

# C Proprietary and Established Names

Epredia E1000 Dx Digital Pathology Solution

D Regulatory Information

<table><tr><td rowspan=1 colspan=1>ProductCode(s)</td><td rowspan=1 colspan=1>Classification</td><td rowspan=1 colspan=1>RegulationSection</td><td rowspan=1 colspan=1>Panel</td></tr><tr><td rowspan=1 colspan=1>PSY</td><td rowspan=1 colspan=1>Class II</td><td rowspan=1 colspan=1>21 CFR 864.3700</td><td rowspan=1 colspan=1>88-Pathology</td></tr></table>

II Submission/Device Overview:

A Purpose for Submission: New Device   
B Type of Test: Digital pathology whole slide imaging

# III Intended Use/Indications for Use:

# A Intended Use(s):

The Epredia $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution is an automated digital slide creation, viewing, and management system. The Epredia $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution is intended for in vitro diagnostic use as an aid to the pathologist to review and interpret digital images of surgical pathology slides prepared from formalin-fixed paraffin embedded (FFPE) tissue. The Epredia E1000 Dx Digital Pathology Solution is not intended for use with frozen section, cytology, or non-FFPE hematopathology specimens.

The Epredia $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution consists of a Scanner $( \mathrm { E 1 0 0 0 D x }$ Digital Pathology Scanner), which generates images in MRXS image file format, $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Scanner Software, Image Management System (E1000 Dx IMS), $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Viewer Software, and Display (Barco MDPC-8127). The Epredia E1000 Dx Digital Pathology Solution is for creation and viewing of digital images of scanned glass slides that would otherwise be appropriate for manual visualization by conventional light microscopy. It is the responsibility of a qualified pathologist to employ appropriate procedures and safeguards to assure the validity of the interpretation of images obtained using Epredia E1000 Dx Digital Pathology Solution.

# B Indication(s) for Use:

Same as Intended Use.

# C Special Conditions for Use Statement(s):

Rx - For Prescription Use Only For In vitro diagnostic (IVD) use only.

# IV Device/System Characteristics:

# A Device Description:

The $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution is an automated whole slide imaging system for the creation, viewing, and management of digital images of surgical pathology slides, which consists of the following three components:

Scanner component:

• E1000 Dx Digital Pathology Scanner with E1000 firmware version 2.0.3 E1000 Dx Scanner Software version 2.0.3

Viewer component:

E1000 Dx Image Management System (IMS) Server version 2.3.2   
E1000 Dx Viewer Software version 2.7.2

Display component: • Barco MDPC-8127

The $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution creates digital whole slide images (WSIs) by scanning FFPE tissue slides. The user loads prepared glass slides into commercially available (Sakura Tissue-Tek® 20-Slide Basket) slide baskets which holds a maximum of 20 slides. The total capacity of the system is 50 baskets, resulting in a capacity of 1,000 slides.The E1000 Dx Scanner Software (EDSS) runs on the scanner workstation and controls the operation of the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Scanner. The scanner workstation, provided with the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution, includes a PC, monitor, keyboard, and mouse. The EDSS user interface allows the user to select baskets conatining slides for scanning and generation of WSIs. The system will only scan areas of the glass slide where tissue sample is present using a machine learning model trained on annotated samples to automatically detect the presence and location of the tissue sample on the glass slide. Once the sample has been located, the system generates a scan-map from the data. FFPE tissue mounted on glass slides are scanned at a resolution of at least $0 . 2 4 \mu \mathrm { m }$ per pixel using a $2 0 \mathrm { x }$ objective. The system acquires images in a square matrix pattern. The device uses a proprietary MRXS format to store and transmit images between the

E1000 Dx Digital Pathology Scanner and the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Image Management System (IMS). MRXS images displayed in $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Viewer are compressed to $80 \%$ of the source MRXS file, or JPEG $\mathrm { Q } = 8 0$ .

The E1000 Dx IMS is a software only component which contains two modules hosted on the IMS server: $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ IMS (allows the pathologist to manage patient cases from digitization through to pathologist assignment and viewing of images) and $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Storage (server software for storing and managing WSIs created by the E1000 scanner and provides background processes for storing and managing the digital slides to be used within $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ IMS and E1000 Dx Viewer). IMS local server is connected to the Scanner Workstation on a high-speed local LAN network. The server runs all IMS services and applications as well as authentication and logging services. A user can access the IMS through a web browser via the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ IMS web application user interface (UI) and open slides in the E1000 Dx Viewer software from a customer provided computer environment.

The $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Viewer is a Windows application that allows viewing and annotating MRXS digital slides created by the E1000 Whole Slide Imaging System. It is integrated with the E1000 Dx IMS and it cannot be used as a standalone application. The $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Viewer can only be launched from the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ IMS web application by clicking on a digital slide attached to a case. The main features of the viewing software are:

• Display image with seamless zooming and panning • Record and recall image annotations • Export image snapshots • Display multiple slides from the same case side by side

The Barco MDPC-8127 display (cleared under K203364) allows the WSIs images to be viewed.

# B Instrument Description Information:

1. Instrument Name: Epredia E1000 Dx Digital Pathology Solution

2. Specimen Identification:

Glass slides and scanned images are identified based on the previously assigned specimen identifiers such as patient identifiers, barcodes, etc. Digital images of surgical pathology slides prepared from FFPE tissue. The preview image captures the barcode information on the slide, which links the scanned slides to the correct entry in the E1000 Dx IMS.

3. Specimen Sampling and Handling:

Specimen sampling and handling are performed independently of the system. Specimen sampling includes surgical pathology specimens such as biopsy or resection specimens which are processed using standard histology techniques. The FFPE tissue sections are stained using the Hematoxylin and Eosin (H&E) staining procedure. Then digital images are obtained from these glass slides using the Epredia E1000 Dx Digital Pathology scanner.

# 4. Calibration:

The E1000 Dx Digital Pathology Scanner is factory calibrated. No calibration is required by the operator of the instrument. The spectral properties of components in the pixel pathway, including the light source, are stable by design, so no regular color recalibration is needed.

Image intensity might vary from slide to slide depending on the slide thickness, and the system automatically adjusts for it. The E1000 Dx IMS subsystem does not need calibration. The Barco MDPC-8127 8MP 27” Pathology Monitor Display is automatically calibrated.

# 5. Quality Control:

Before loading the slide to the instrument, the operator ensures the slide is within the specifications of the instrument and laboratory standards such as slide size, coverslip size and placement, barcode placement, staining quality, etc. After scanning a lab technician checks if the WSI quality and rescans the slide(s) as needed.

Before reading the scanned WSIs, the pathologist should ensure that all scanned slide images have been imported for every case and the images are of acceptable quality for diagnostic purposes. The pathologist reviews scanned images of all slides associated with a case before rendering a diagnosis.

V Substantial Equivalence Information:

A Predicate Device Name(s): Philips IntelliSite Pathology Solution (PIPS)   
B Predicate 510(k) Number(s): DEN160056   
C Comparison with Predicate(s):

<table><tr><td colspan="1" rowspan="1">Device &amp; PredicateDevice(s):</td><td colspan="1" rowspan="1">K241717</td><td colspan="1" rowspan="1">DEN160056</td></tr><tr><td colspan="1" rowspan="1">Device Trade Name</td><td colspan="1" rowspan="1">Epredia E1000 Dx DigitalPathology Solution</td><td colspan="1" rowspan="1">Philips IntelliSite PathologySolution (PIPS)</td></tr><tr><td colspan="3" rowspan="1">General Device Characteristic Similarities</td></tr><tr><td colspan="1" rowspan="1">IntendedUse/Indications ForUse</td><td colspan="1" rowspan="1">The Epredia E1000 Dx DigitalPathology Solution is anautomated digital slide creation,viewing, and management system.The Epredia E1000 Dx DigitalPathology Solution is intended forin vitro diagnostic use as an aid tothe pathologist to review andinterpret digital images of surgicalpathology slides prepared fromformalin-fixed paraffin embedded(FFPE) tissue. The Epredia E1000Dx Digital Pathology Solution isnot intended for use with frozensection, cytology, or non-FFPEhematopathology specimens.</td><td colspan="1" rowspan="1">The Philips IntelliSite PathologySolution (PIPS) is an automateddigital slide creation, viewing,and management system. ThePIPS is intended for in vitrodiagnostic use as an aid to thepathologist to review andinterpret digital images ofsurgical pathology slidesprepared from formalin-fixedparaffin embedded (FFPE)tissue. The PIPS is not intendedfor use with frozen section,cytology, or non-FFPEhematopathology specimens.The PIPS comprises the Image</td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1">The Epredia E1000 Dx DigitalPathology Solution consists of aScanner (E1000 Dx DigitalPathology Scanner), whichgenerates images in MRXS imagefile format, E1000 Dx ScannerSoftware, Image ManagementSystem (E1000 Dx IMS), E1000Dx Viewer Software, and Display(Barco MDPC-8127). The EprediaE1000 Dx Digital PathologySolution is for creation andviewing of digital images ofscanned glass slides that wouldotherwise be appropriate formanual visualization byconventional light microscopy. Itis the responsibility of a qualifiedpathologist to employ appropriateprocedures and safeguards toassure the validity of theinterpretation of images obtainedusing Epredia E1000 Dx DigitalPathology Solution.</td><td colspan="1" rowspan="1">Management System (IMS), theUltra-Fast Scanner (UFS) andDisplay. The PIPS is for creationand viewing of digital images ofscanned glass slides that wouldotherwise be appropriate formanual visualization byconventional light microscopy.It is the responsibility of aqualified pathologist to employappropriate procedures andsafeguards to assure the validityof the interpretation of imagesobtained using PIPS.</td></tr><tr><td colspan="1" rowspan="1">Device Components</td><td colspan="1" rowspan="1">WSI Scanner (E1000 Dx DigitalPathology Scanner), ImageManagement System (E1000 DxIMS), Image Viewer (E1000 DxViewer) and color monitordisplay.</td><td colspan="1" rowspan="1">PIPS Ultra Fast Scanner, ImageManagement System and colormonitor display.</td></tr><tr><td colspan="1" rowspan="1">ScanningMagnification</td><td colspan="1" rowspan="1">0.25 μm per pixel</td><td colspan="1" rowspan="1">0.24 µm per pixel</td></tr><tr><td colspan="1" rowspan="1">Principles ofOperation</td><td colspan="1" rowspan="1">The technician performs qualitycontrol procedures per thelaboratory's standard and loadssurgical pathology slides, preparedfrom FFPE tissue, into the EprediaDx Digital Scanner. Subsequently,the scanner scans the slides,generating WSIs from each slide.The resulting WSI images arestored in the image storage systemincluded in the device. During thereview process, the pathologistuses the IMS to open the E1000Dx Viewer to view the images</td><td colspan="1" rowspan="1">Prior to scanning the slide on theUltra Fast Scanner (UFS), thetechnician conducts qualitycontrol on the slides preparedfrom FFPE tissue per thelaboratory's standards. Thetechnician places the slides theninto slide racks, loads the racksinto the UFS. The imagesscanned in the UFS aretransmitted to the IMSsubsystem. The pathologistaccesses WSI images acquiredthrough the UFS from the image</td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1">from the image storage andproceeds to analyze the WSIimages of the slides.</td><td colspan="1" rowspan="1">storage and proceeds to analyzethe WSI images of the slides.</td></tr><tr><td colspan="3" rowspan="1">General Device Characteristic Differences</td></tr><tr><td colspan="1" rowspan="1">Image ReviewManipulationFunctionality</td><td colspan="1" rowspan="1">Continuous panning and pre-fetching, continuous zoomingcapability to compare multipleslides simultaneously on multiplewindows, image enhancement andcolor manipulation, and annotationtools. Sharpening is done on thescanner side.</td><td colspan="1" rowspan="1">Continuous panning and pre-fetching, continuous zooming,discrete Z-axis displacement,ability to compare multipleslides simultaneously onmultiple windows, imageenhancement and sharpeningfunctions, color manipulation,annotation tools, digitalbookmarks, and virtual multi-head microscope.</td></tr><tr><td colspan="1" rowspan="1">Image File Format</td><td colspan="1" rowspan="1">Proprietary format MRXS is usedto store and transmit imagesbetween the E1000Dx Scannerand the IMS.</td><td colspan="1" rowspan="1">Philips's proprietary format,iSyntax, is used to store andtransmit the images between theUFS and the IMS.</td></tr><tr><td colspan="1" rowspan="1">Slide feeder</td><td colspan="1" rowspan="1">1000 slides (50 glass slide rackswith up to 20 slides per rack).</td><td colspan="1" rowspan="1">300 slides (15 glass slide rackswith up to 20 slides per rack).</td></tr><tr><td colspan="1" rowspan="1">Display</td><td colspan="1" rowspan="1">Barco MDPC-8127</td><td colspan="1" rowspan="1">Philips PP27QHD (BarcoMMPC-4227F1)</td></tr></table>

# VI Standards/Guidance Documents Referenced:

1. FDA guidance “Technical Performance Assessment of Digital Pathology Whole Slide Imaging Devices," dated April 20, 2016.   
2. Applying Human Factors and Usability Engineering to Medical Devices: Guidance for Industry and Food and Drug Administration Staff February 3, 2016.   
3. ISO 13485 Third Edition 2016-03 - Medical devices — Quality management systems — Requirements for regulatory purposes.   
4. ISO 14971 Third edition 2019-12 - Medical devices — Application of risk management to medical devices.   
5. IEC 62304 Edition 1.1 2015-06 - Medical device software — Software life cycle processes.   
6. IEC 61010-1 Edition 3.1 2017-01 - Safety requirements for electrical equipment for measurement, control, and laboratory use - Part 1: General requirements.   
7. IEC 61010-2-101 Third Edition 2018-10 - Safety requirements for electrical equipment for measurement, control, and laboratory use - Part 2-101: Particular requirements for in vitro diagnostic (IVD) medical equipment.   
8. IEC 61326-1: 2020 - Electrical equipment for measurement, control, and laboratory use - EMC requirements - Part 1: General requirements.   
9. IEC 61326-2-6: 2020 - Electrical equipment for measurement, control and laboratory useEMC Requirements. Part 2-6: Particular requirements- In vitro diagnostic (IVD) medical equipment.   
10. IEC 62366-1:2015 +A1:2020 - Medical devices — Part 1: Application of usability engineering to medical devices — Amendment 1   
11. IEC 60825-1:2014 - Safety of laser products - Part 1: Equipment classification and requirements.   
12. IEC 62471:2006-07 - Photobiological safety of lamps and lamp systems (including LEDs)   
13. IEC 63000:2018 - Technical documentation for the assessment of electrical and electronic products with respect to the restriction of hazardous substance.   
14. ISO 15223-1 Fourth edition 2021-07 - Medical devices — Symbols to be used with information to be supplied by the manufacturer — Part 1: General requirements.   
15. ISO 18113-1:2011- In vitro diagnostic medical devices Information supplied by the manufacturer (labelling) — Part 1: Terms, definitions, and general requirements.   
16. ISO 18113-3:2011 In vitro diagnostic medical devices Information supplied by the manufacturer (labelling) — Part 3: In vitro diagnostic instruments for professional use.   
17. ISO 20417: 2021 - Medical devices - Information to be supplied by the manufacturer.

# VII Performance Characteristics (if/when applicable):

# A. Analytical Performance:

1. Precision/Reproducibility:

The objective of this study was to evaluate the repeatability (within- and between- system) and reproducibility (between-site) of the E1000 Dx Digital Pathology Solution.

The precision of the device was evaluated based on the review and identification of specific histopathologic “features” that are observed in FFPE H&E stained slides. Features selected for the study included tissue level features, cellular level features and sub-cellular level features. The study included 279 “study” features and 42 additional "wild card" features at different magnifications (10x, 20x, 40x) from 257 glass slides as shown in Table 1 below. Wild card features were used to reduce the risk of reader recall bias and were not used in the analysis. A minimum of 5 different features were evaluated at each relevant magnification level. Three scanning sites, 3 reading sites and a total of 9 US board-certified reading pathologists (RPs) were used in this study.

Table 1: Primary Histologic Study Features in Precision Study   

<table><tr><td colspan="1" rowspan="1">10x Magnification (n)</td><td colspan="1" rowspan="1">20x Magnification (n)</td><td colspan="1" rowspan="1">40x Magnification (n)</td></tr><tr><td colspan="1" rowspan="1">Tissue level Feature</td><td colspan="1" rowspan="1">Cellular level Feature</td><td colspan="1" rowspan="1">Subcellular level Feature</td></tr><tr><td colspan="1" rowspan="1">Adipocytes (16)</td><td colspan="1" rowspan="1">Clear Cells (18)</td><td colspan="1" rowspan="1">Pigment (17)</td></tr><tr><td colspan="1" rowspan="1">Small Artery (16)</td><td colspan="1" rowspan="1">Duct (17)</td><td colspan="1" rowspan="1">Macrophages/Histocytes (17)</td></tr><tr><td colspan="1" rowspan="1">Nerve (16)</td><td colspan="1" rowspan="1">Lymphoid Aggregate (16)</td><td colspan="1" rowspan="1">Intercellular Bridges (16)</td></tr><tr><td colspan="1" rowspan="1">Necrosis (15)</td><td colspan="1" rowspan="1">Myxoid Stroma (16)</td><td colspan="1" rowspan="1">Lobular Carcinoma (15)</td></tr><tr><td colspan="1" rowspan="1">Squamous Epithelium (16)</td><td colspan="1" rowspan="1">Goblet Cells (16)</td><td colspan="1" rowspan="1">Nucleoli (14)</td></tr><tr><td colspan="1" rowspan="1">Gland (16)</td><td colspan="1" rowspan="1">Calcification (12)</td><td colspan="1" rowspan="1">Cilia (13)</td></tr><tr><td colspan="1" rowspan="1">Muscle (16)</td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1">Nuclear Pleomorphism (12)</td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1">Keratin Pearl (11)</td></tr><tr><td colspan="1" rowspan="1">Study Feature =111Wild ard Feature = 15</td><td colspan="1" rowspan="1">Study Feature =95Wild Card Feature =12</td><td colspan="1" rowspan="1">Study Feature =115Wild Card Feature = 15</td></tr></table>

Consecutive cases were selected from the pathology laboratory using the laboratory information system (LIS) by an enrollment pathologist (EP). A different validating enrollment pathologist (VEP) confirmed whether the feature was present on the glass slide. Slide scanning of the same set of enrolled slides was performed sequentially at all three scanning sites. Three rescans, as needed, of each slide was allowed. Once the slides were scanned, the EP reviewed the image and defined an area (bookmark) containing the selected feature(s) at the appropriate magnification. Then a static full resolution extraction image of the bookmark was created and defined as the Field of View (FOV). The VEP confirmed whether the feature(s) was present in the FOV. After VEP confirmation, the FOV was considered enrolled into the study. These FOV images were randomly rotated in 90-degree increments and randomly assigned to the reading pathologists (RPs) to minimize recall bias. The reading pathologist (RP) at each site recorded the presence of each observed feature on a checklist. For each magnification, a separate checklist containing features was developed with a multiple answer list of up to 9 possible answers. There was a minimum washout period of 14 days between pathologist reading sessions. Only the primary features were used in the data analysis.

The precision of the Epredia E1000 Dx Digital Pathology Solution was assessed in the following studies:

Intra-System (Within-System) precision was assessed using 3 independent systems. Overall within system precision was also assessed. Inter-System (Between-System) precision was assessed using 3 independent systems at 3 different sites.   
• Within- and Between-pathologist precision was assessed using images generated from a single system.

Study acceptance criteria: The precision was considered acceptable if the lower bounds of the 2- sided $9 5 \%$ confidence interval (CI) of the overall agreements for each precision component (within-system, between-system/site, within-pathologist, and between-pathologist) were $\ge 8 5 \%$ .

In total, 321 features $3 \mathrm { ~ x ~ } 9 3$ study features $+ 4 2$ wild card features) were used. Each of 279 study slides had at least one primary feature. The study slide set was divided equally $( { \mathfrak { n } } = 9 3$ study slides $+ 4 2$ wild card slides) over 3 systems. Each slide set was scanned three times with at least six hours between scanning iterations on each system. From these scans, a total of 405 FOVs (279 study $+ \ 1 2 6$ wild card) were extracted.

Randomly selected FOVs originating from three different systems and three different iterations were read by each of three RPs for a total of 3,645 [ $9 3 + 4 2$ FOVs) x 3 systems x 3 scans x 3 Pathologists] reads. The reading pathologist at the site recorded the presence of each observed feature on a checklist for each pre-determined magnification level. Data analysis included 2,511 (93 study FOVs x 3 systems x 3 scans x 3 Pathologists) reads out of 3,645 reads.

Study results are shown in Table 2 below. The data showed that the acceptance criteria of a lower limit of the $9 5 \%$ Confidence interval (CI) greater than $8 5 \%$ , with an Average Positive Agreement (APA) of $9 6 . 9 \%$ (lower limit of $9 6 . 1 \%$ ) was met.

Table 2: Within Systems Precision Study Results   

<table><tr><td rowspan=2 colspan=1>System</td><td rowspan=2 colspan=1>Number of PairwiseAgreements*</td><td rowspan=2 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=2>Agreement Rate</td></tr><tr><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>System 1</td><td rowspan=1 colspan=1>820</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>98.0</td><td rowspan=1 colspan=1>(96.8, 98.8)</td></tr><tr><td rowspan=1 colspan=1>System 2</td><td rowspan=1 colspan=1>803</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>95.9</td><td rowspan=1 colspan=1>(94.5, 97.3)</td></tr><tr><td rowspan=1 colspan=1>System 3</td><td rowspan=1 colspan=1>810</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>96.8</td><td rowspan=1 colspan=1>(95.4, 97.9)</td></tr><tr><td rowspan=1 colspan=1>Total</td><td rowspan=1 colspan=1>2,433</td><td rowspan=1 colspan=1>2,511</td><td rowspan=1 colspan=1>96.9</td><td rowspan=1 colspan=1>(96.1, 97.6)</td></tr></table>

# Between-System Precision Study

The between-system precision study utilized the full set of glass slides from the within-system study, including the same extracted FOVs, primary study features and wildcard features. Each slide was scanned once on each system. Three reading pathologists then evaluated each enrolled FOV once on each system. The pathologists recorded the presence of each observed feature on a checklist for each magnification level. Each RP read 321 FOVs (279 Study FOVs, 42 wild card FOVs) in each of three (3) different reading sessions. There was a total of 2,889 reads [(279 study slides $+ 4 2$ wild card slides) $\textbf { X } 3$ systems $_ { \textrm { x 1 } }$ times x 3 Pathologists] and 378 of wild-card FOVs readings were excluded. There were 2,511 pairwise comparisons used for calculating the agreement rates.

Study results are shown in Table 3 below. The data showed that the acceptance criteria of a lower limit of $9 5 \%$ CI greater than $8 5 \%$ , with an APA of $9 5 . 1 \%$ (lower limit of $9 4 \%$ ) was met.

Table 3: Between-System Precision Study Results   

<table><tr><td rowspan=2 colspan=1>System</td><td rowspan=2 colspan=1>Number of PairwiseAgreements</td><td rowspan=2 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=2>Agreement Rate</td></tr><tr><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>System 1 vs 2</td><td rowspan=1 colspan=1>788</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>94.1</td><td rowspan=1 colspan=1>(91.3, 96.0)</td></tr><tr><td rowspan=1 colspan=1>System 1 vs 3</td><td rowspan=1 colspan=1>793</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>94.7</td><td rowspan=1 colspan=1>(92.6, 96.5)</td></tr><tr><td rowspan=1 colspan=1>System 2 vs 3</td><td rowspan=1 colspan=1>808</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>96.5</td><td rowspan=1 colspan=1>(94.7, 97.9)</td></tr><tr><td rowspan=1 colspan=1>Total</td><td rowspan=1 colspan=1>2,389</td><td rowspan=1 colspan=1>2,511</td><td rowspan=1 colspan=1>95.1</td><td rowspan=1 colspan=1>(94.1, 96.1)</td></tr></table>

# Between-Site Reproducibility

For the between-site reproducibility study, the FOVs from the within-system and betweensystem precision studies were used as the study FOVs. The study involved 3 different RPs, each located at one of 3 different sites, each equipped with its own $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution. The order of FOV evaluation at each site was randomly determined. Each pathologist evaluated each FOV once and recorded the presence of each observed feature on a checklist for each magnification level.

Study results are shown in Table 4 below. The data showed that the acceptance criteria of a lower limit of $9 5 \%$ CI greater than $8 5 \%$ , with an APA of $9 5 . 4 \%$ (lower limit of $9 3 . 5 \%$ ) was met.

Table 4: Between-Site Reproducibility Agreement Rates   

<table><tr><td rowspan=2 colspan=1>System</td><td rowspan=2 colspan=1>Number of PairwiseAgreements</td><td rowspan=2 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=2>Agreement Rate</td></tr><tr><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Site 1 vs 2</td><td rowspan=1 colspan=1>261</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>93.5</td><td rowspan=1 colspan=1>(89.4, 96.7)</td></tr><tr><td rowspan=1 colspan=1>Site 1 vs 3</td><td rowspan=1 colspan=1>268</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>96.1</td><td rowspan=1 colspan=1>(92.5, 98.4)</td></tr><tr><td rowspan=1 colspan=1>Site 2 vs 3</td><td rowspan=1 colspan=1>270</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>96.8</td><td rowspan=1 colspan=1>(93.3, 98.8)</td></tr><tr><td rowspan=1 colspan=1>Total</td><td rowspan=1 colspan=1>799</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>95.4</td><td rowspan=1 colspan=1>(93.6, 97.2)</td></tr></table>

# Between-Pathologist Reproducibility

Each RP read 321 FOVs (279 Study FOVs, 42 wild card FOVs) in each of the 3 different reading sessions. There were a total 2,889 reads (963 reads per RP), and 378 of wild-card FOVs readings were excluded from the analysis. A total of 2,511 pairwise comparisons were used for calculating the agreement rates.

Study results are shown in Table 5 below. The data show that the study acceptance criterion of the lower limit of the $9 5 \%$ confidence interval of the $\%$ of agreement rate exceeding $8 5 \%$ was met.

Table 5: Between-Pathologist Reproducibility   

<table><tr><td rowspan=2 colspan=1>System</td><td rowspan=2 colspan=1>Number of PairwiseAgreements</td><td rowspan=2 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=2>Agreement Rate</td></tr><tr><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Pathologist 1 vs 2</td><td rowspan=1 colspan=1>818</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>97.7</td><td rowspan=1 colspan=1>(96.6, 98.7)</td></tr><tr><td rowspan=1 colspan=1>Pathologist1vs 3</td><td rowspan=1 colspan=1>813</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>97.1</td><td rowspan=1 colspan=1>(95.9, 98.3)</td></tr><tr><td rowspan=1 colspan=1>Pathologist 2 vs 3</td><td rowspan=1 colspan=1>816</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>97.5</td><td rowspan=1 colspan=1>(96.4, 98.5)</td></tr><tr><td rowspan=1 colspan=1>Total</td><td rowspan=1 colspan=1>2,447</td><td rowspan=1 colspan=1>2,511</td><td rowspan=1 colspan=1>97.5</td><td rowspan=1 colspan=1>(96.9, 98.1)</td></tr></table>

# Within-Pathologist Reproducibility

Each RP read 321 FOVs (279 Study FOVs, 42 wild card FOVs) in each of 3 different reading sessions. There was a total of 2,889 reads (963 reads per RP), and 378 wild-card FOVs readings were excluded from the analysis. Agreement pairs were scored for each pathologist between repeated scans (Scan 1 vs 2, Scan 1 vs 3, and Scan 2 vs 3) across all 3 systems. Totals for each pathologist as well as aggregated data from all pathologists was also calculated.

Study results are shown in Table 6 below. The data show that the study acceptance criterion of the lower limit of the $9 5 \%$ confidence interval of the $\%$ of agreement rate exceeding $8 5 \%$ was met.

Table 6: Between-Pathologist Reproducibility   

<table><tr><td rowspan=2 colspan=1>System</td><td rowspan=2 colspan=1>Number of PairwiseAgreements</td><td rowspan=2 colspan=1>Number ofComparison Pairs</td><td rowspan=1 colspan=2>Agreement Rate</td></tr><tr><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>RP 1, System 1</td><td rowspan=1 colspan=1>269</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>96.4</td><td rowspan=1 colspan=1>(93.6, 98.4)</td></tr><tr><td rowspan=1 colspan=1>RP 1, System 2</td><td rowspan=1 colspan=1>269</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>96.4</td><td rowspan=1 colspan=1>(93.6, 98.4)</td></tr><tr><td rowspan=1 colspan=1>RP 1, System 3</td><td rowspan=1 colspan=1>265</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>95.0</td><td rowspan=1 colspan=1>(91.9 97.3)</td></tr><tr><td rowspan=1 colspan=1>RP1 (Total)</td><td rowspan=1 colspan=1>803</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>95.9</td><td rowspan=1 colspan=1>(94.4, 97.2)</td></tr><tr><td rowspan=1 colspan=1>RP 2, System 1</td><td rowspan=1 colspan=1>271</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>97.1</td><td rowspan=1 colspan=1>(94.5, 98.8)</td></tr><tr><td rowspan=1 colspan=1>RP 2, System 2</td><td rowspan=1 colspan=1>269</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>96.4</td><td rowspan=1 colspan=1>(93.7, 98.4)</td></tr><tr><td rowspan=1 colspan=1>RP 2, System 3</td><td rowspan=1 colspan=1>267</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>95.7</td><td rowspan=1 colspan=1>(92.6, 97.9)</td></tr><tr><td rowspan=1 colspan=1>RP 2 (Total)</td><td rowspan=1 colspan=1>807</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>96.4</td><td rowspan=1 colspan=1>(94.9, 97.7)</td></tr><tr><td rowspan=1 colspan=1>RP 3, System 1</td><td rowspan=1 colspan=1>271</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>97.1</td><td rowspan=1 colspan=1>(94.5, 98.8)</td></tr><tr><td rowspan=1 colspan=1>RP 3, System 2</td><td rowspan=1 colspan=1>267</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>95.7</td><td rowspan=1 colspan=1>(92.6, 97.8)</td></tr><tr><td rowspan=1 colspan=1>RP 3, System 3</td><td rowspan=1 colspan=1>269</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>96.4</td><td rowspan=1 colspan=1>(93.6, 96.5)</td></tr><tr><td rowspan=1 colspan=1>RP 3 (Total)</td><td rowspan=1 colspan=1>807</td><td rowspan=1 colspan=1>837</td><td rowspan=1 colspan=1>96.4</td><td rowspan=1 colspan=1>(95.0, 97.6)</td></tr><tr><td rowspan=1 colspan=1>Overall</td><td rowspan=1 colspan=1>2417</td><td rowspan=1 colspan=1>2,511</td><td rowspan=1 colspan=1>96.3</td><td rowspan=1 colspan=1>(95.5, 97.2)</td></tr></table>

2. Linearity:

Not applicable

3. Analytical Specificity/Interference: Not applicable

4. Accuracy (Instrument): Not applicable

5. Carry-Over: Not applicable

# B. Technical Studies:

Multiple studies were conducted to evaluate the performance of the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution as recommended in FDA Guidance, “Technical Performance Assessment of Digital Pathology Whole Slide Imaging Devices”.

a. Slide Feeder

The slide feeder mechanism was described, including a physical description of the slide, the number of slides in queue (carrier), and the class of automation. The user interaction with the slide feeder was also described, including hardware, software, and feedback mechanisms.

# b. Light Source

Descriptive information about the lamp and the condenser was provided. Testing information was provided to verify the spectral distribution of the light source as part of the color reproduction capability of the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Scanner subsystem.

c. Imaging Optics

An optical schematic was provided, showing all optical elements from the slide (object plane) to the digital image sensor (image plane). Descriptive information about the microscope objective, the auxiliary lenses, and the magnification of the imaging optics was also provided. Testing information about the relative irradiance, image distortions, and lateral chromatic aberrations was also provided.

# d. Mechanical Scanner Movement

Information and specifications about the configuration of the stage, the method of movement, and the control of movement of the stage were provided. Test data to verify the repeatability of the stage movement and to verify that the stage movement stays within limits during operations was also provided.

# e. Digital Imaging Sensor

Information and specifications about the sensor type, pixel information, responsivity specifications, noise specifications, readout rate, and digital output format were provided. Vendor provided test data to determine the correct functioning of the digital image sensor, which converts optical signals of the slide to digital signals that consist of a set of numerical values corresponding to the brightness and color at each point in the optical image, was also provided.

# f. Image Processing Software

Information and specifications on the exposure control, white balance, color correction, and flat-field correction was provided.

g. Image Composition

Information and specifications on the scanning method was provided. Test data to analyze the image composition performance was provided.

h. Image Files Format

Information and specifications on the compression method, compression ratio, file format, and file organization were provided.

i. Image Review Manipulation Software

Information and specifications on continuous panning and pre-fetching, continuous zooming, discrete Z-axis displacement, ability to compare multiple slides simultaneously on multiple windows, image enhancement and sharpening functions, color manipulation, annotation tools, digital bookmarks, and virtual multihead microscope was provided.

# $j$ . Computer Environment

Information and specifications on the computer hardware, operating system, graphics card, graphics card driver, color management settings, color profile, and display interface was provided.

# k. Display

Information and specifications on the technological characteristics of the display device, physical size of the viewable area and aspect ratio, backlight type and properties, frame rate and refresh rate, pixel pitch, and supported color spaces, color depth, display interface, user

controls of brightness, contrast ratio, gamma, color space, etc., via the on-screen display menu, ambient light adaptation, and color calibration tools was provided.

# l. Color Reproducibility

Test data to evaluate the color reproducibility of the system was provided.

m. Spatial Resolution

Test data to evaluate the composite optical performance of all components in the image acquisition phase was provided.

n. Focusing Test Test data to evaluate the technical focus quality of the system was provided.

o. Whole Slide Tissue Coverage Test data to demonstrate that the entire tissue specimen on the glass slide is detected by the tissue detection algorithms and that all the tissue specimens are included in the digital image file was provided.

$p$ . Stitching Error

Test data to evaluate the stitching errors and artifacts in the reconstructed image was provided.

# q. Turnaround Time

Test data to evaluate the turnaround time of the system was provided.

# C. Clinical Study:

A retrospective, multi-center study was conducted to demonstrate that viewing, reviewing, and diagnosing digital images of surgical pathology FFPE tissue slides using the Epredia E1000 Dx Digital Pathology Solution is non-inferior to using traditional optical light microscopy. The primary endpoint was the difference in agreement rates between diagnoses rendered using Epredia $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution’s digital WSI review modality [Manual Digital (MD)] and the manual microscopy slide review modality [Manual Optical (MO)] when each was compared to the reference diagnosis, which is based on the original main sign-out diagnosis (SD) rendered at the study sites using an optical light microscope. The study consisted of reviewing archived, de-identified and previously “signed-out” slides representing main organ systems within surgical pathology. Cases included retrospective H&E stained FFPE tissue, special stains and/or immunohistochemical stains (IHC) from the pathology practice, but did not include frozen sections, or cytological and hematological cases.

The study included 1,299 consecutively enrolled cases containing 1796 individual slides. These cases were at least one year old for which a sign-out diagnosis is available at 3 enrollment sites. These slides were scanned in one site using the Epredia $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ slide scanner. Slides corresponding to each case were selected by the Enrollment Pathologist (EP) from the original slide(s) used for the main (sign-out) diagnosis and then given a case identifier using a barcode label and randomized. The EP reviewed the pathology report for each case and determined the main diagnosis for each case. The EP then matched the case to the list of types of cases to be evaluated and selected the representative slide(s) that reflected the main diagnosis.

Cases were randomly distributed to 3 reading sites. At site one, 24 pathologists were divided into 3 groups with each group consisting of pathologists representing 10 subspecialties. Some pathologists had expertise in two subspecialties. Cases within each subspecialty were read three times by the corresponding specialists at site one, distributed across the three groups for their respective subspecialties. At sites two and three, 3 RPs read every case assigned to the site. Each case was read 3 times in an alternating and randomized order, with a washout period of at least 30 days between reads, resulting in a case total of 3,897 WSI reads and 3,881 Glass reads.

# Study Inclusion Criteria

• The case is at least one year for which a sign-out diagnosis is available.   
• Formalin Fixed Paraffin Embedded (FFPE) tissue slides.   
• All glass slides, with human tissue obtained via surgical pathology of original case are available.   
• The original sign-out diagnosis and ancillary supporting information is available.   
• The selected slide or slides for the main diagnosis and the control slide(s) fulfill quality checks according to general practice.   
• Case, including cases previously referred to another site, for which Hematoxylin and Eosin (H&E), Immunohistochemistry (IHC) or special stains slides used for the original sign-out diagnosis is available at the site.   
• The selected slide (s) for the main diagnosis match any subtype of the organ for which the case is selected.

# Study Exclusion Criteria

• Cases that were signed out less than one year prior to the date of curation.   
• Case with an amended report resulting in a change in diagnosis.   
• Case with slides containing indelible markings.   
• Case with slides that are damaged.   
• Glass slide that is broken, has many pen markings or dirt that cannot be removed, has abnormal size/thickness, beveled edges, poor coverslip (cracks, waviness, scratches), is sticky, contains air bubbles and overhanging labels that can’t be corrected, and if the stain is severely faded.   
• More than one case selected (only one case may be enrolled per patient).   
• Case consists of frozen section(s) only.   
• Case consists of gross specimen(s) only.

An independent adjudication panel consisting of 5 Adjudication Pathologists (APs) independent of the study were utilized for this study to determine if the study diagnosis to consistent with the reference (sign-out) diagnosis. Four of the 5 APs were divided into 2 Adjudication Groups (AGs) of 2 APs each. The remaining 1 AP was assigned as tiebreaker Adjudication Pathologist (TAP). Each CRF was independently reviewed by both 2 APs of 1 of the 2 AGs to determine whether the diagnosis is consistent with the sign-out (main) diagnosis. A major discordance was defined as a difference in diagnosis that would be associated with a clinically important difference in patient management. A minor discordance was defined as a difference in diagnosis that would not be associated with a clinically important difference in patient management. In the event the APs in an AG disagreed, the TAP reviewed the case to achieve majority vote (tiebreaker). In cases where all APs and TAP have a different opinion, consensus was achieved in an adjudication panel meeting consisting of the same three APs.

The observed major discordance rates were observed and calculated using a Generalized Linear Mixed Model (GLIMMIX) logistic regression. For each reading result, the dependent variable was the major discordance status and independent variables included modality as a fixed effect (MD vs MO) and site, reader, and case as random effects. A two-sided $9 5 \%$ Confidence Interval (CI) for the overall discordance between reading modalities (MD-MO) was calculated from this analysis.

Out of the 1,299 cases included in the study, 5 cases were removed from the study and therefore did not undergo MO review Due to the following reasons: Two cases were recalled to their original clinics as the patients were readmitted and the slides were requested by the primary physicians and slides from 3 cases were damaged during transport and were not able to be read. The reading sessions resulted in a total of 7,778 reading pairs (3,897 WSI and 3,881 Glass). In total, 3,897 (1299 cases x 3 pathologist) readings of WSI cases and $^ { 3 , 8 8 1 ( ^ { * } ) }$ readings of glass slide cases were adjudicated [\*final cases (glass) reviewed $( 1 2 9 9 - 5 ) { = } 1 2 9 4 ;$ 1 case was reviewed by 2 out of 3 pathologists due to a slide breaking during transit, which resulted in 3881 glass slide case reads [(1293 cases x 3 pathologists) $^ +$ (1 case $_ { \textrm { X 2 } }$ pathologists) $\left. \right] = 3 8 8 1 \mathrm { J }$ .

# Study Results

The major discordance rate between WSI and SD (MD) was observed to be $2 . 5 4 \%$ (99/3897) and the major discordance rate between Glass and SD (MO) was observed to be $2 . 6 5 \%$ (103/3881). The overall major discordance rates estimated by the generalized linear model were $2 . 5 1 \%$ ( $9 5 \%$ CI: $2 . 2 6 \%$ ; $2 . 7 9 \%$ ) for MD and $2 . 5 9 \%$ $9 5 \%$ CI: $2 . 2 9 \%$ , $2 . 8 2 \% )$ for MO. The estimated difference in major discordance between the MD and MO rates was calculated to be $- 0 . 1 5 \%$ ( $9 5 \%$ CI: $- 0 . 4 0 \%$ , $0 . 4 1 \%$ ). These results met the acceptance criteria for the study. Specifically, the major discordance rate for WSI (MD), which was $2 . 5 \%$ , was required to be $< 7 . 0 \%$ and the upper limit of the $9 5 \%$ confidence interval of the difference in major discordance rate between WSI and Glass (MD-MO), which was $0 . 4 1 \%$ , was required to be ${ < } 4 . 0 \%$ .

The study results are shown in Table 7 below.

Table 7: Overall Major Discordance Rate for MD and MO   

<table><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3>MD: Major DiscordanceRate</td><td rowspan=1 colspan=3>MO: Major DiscordanceRate</td><td rowspan=1 colspan=2>DifferenceMD - MO</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>N</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td><td rowspan=1 colspan=1>N</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>95% CI</td></tr><tr><td rowspan=1 colspan=1>Observed</td><td rowspan=2 colspan=1>3897</td><td rowspan=1 colspan=1>2.54%</td><td rowspan=1 colspan=1></td><td rowspan=2 colspan=1>3881</td><td rowspan=1 colspan=1>2.65%</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>-0.11%</td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=1>Modeled</td><td rowspan=1 colspan=1>2.51%</td><td rowspan=1 colspan=1>(2.26%;2.79%)</td><td rowspan=1 colspan=1>2.59%</td><td rowspan=1 colspan=1>(2.29%,2.82%)</td><td rowspan=1 colspan=1>-0.15%</td><td rowspan=1 colspan=1>(-0.40%,0.41%)</td></tr></table>

The detailed list of the major discordance rate for each organ system and anatomical region are shown in Table 8 below.

Table 8: Major Discordance Rates Observed Across Organ Systems   

<table><tr><td colspan="1" rowspan="1">Organ System</td><td colspan="2" rowspan="1">MD MajorDiscordance</td><td colspan="2" rowspan="1">MO MajorDiscordance</td><td colspan="1" rowspan="1">MD - MODifference</td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1">N</td><td colspan="1" rowspan="1">%</td><td colspan="1" rowspan="1">N</td><td colspan="1" rowspan="1">%</td><td colspan="1" rowspan="1">% Difference</td></tr><tr><td colspan="1" rowspan="1">Breast</td><td colspan="1" rowspan="1">360</td><td colspan="1" rowspan="1">0.6%</td><td colspan="1" rowspan="1">357</td><td colspan="1" rowspan="1">1.1%</td><td colspan="1" rowspan="1">-0.6%</td></tr><tr><td colspan="1" rowspan="1">Dermatological</td><td colspan="1" rowspan="1">297</td><td colspan="1" rowspan="1">3.4%</td><td colspan="1" rowspan="1">297</td><td colspan="1" rowspan="1">1.7%</td><td colspan="1" rowspan="1">1.7%</td></tr><tr><td colspan="1" rowspan="1">Endocrine</td><td colspan="1" rowspan="1">270</td><td colspan="1" rowspan="1">3.0%</td><td colspan="1" rowspan="1">270</td><td colspan="1" rowspan="1">3.3%</td><td colspan="1" rowspan="1">-0.4%</td></tr><tr><td colspan="1" rowspan="1">Gastrointestinal</td><td colspan="1" rowspan="1">1047</td><td colspan="1" rowspan="1">1.5%</td><td colspan="1" rowspan="1">1037</td><td colspan="1" rowspan="1">1.3%</td><td colspan="1" rowspan="1">0.3%</td></tr><tr><td colspan="1" rowspan="1">Genitourinary</td><td colspan="1" rowspan="1">777</td><td colspan="1" rowspan="1">2.2%</td><td colspan="1" rowspan="1">777</td><td colspan="1" rowspan="1">1.4%</td><td colspan="1" rowspan="1">0.8%</td></tr><tr><td colspan="1" rowspan="1">Gynecological</td><td colspan="1" rowspan="1">357</td><td colspan="1" rowspan="1">7.3%</td><td colspan="1" rowspan="1">357</td><td colspan="1" rowspan="1">10.1%</td><td colspan="1" rowspan="1">-2.8%</td></tr><tr><td colspan="1" rowspan="1">Hematopoietic</td><td colspan="1" rowspan="1">222</td><td colspan="1" rowspan="1">1.8%</td><td colspan="1" rowspan="1">222</td><td colspan="1" rowspan="1">3.2%</td><td colspan="1" rowspan="1">-1.4%</td></tr><tr><td colspan="1" rowspan="1">Pulmonary</td><td colspan="1" rowspan="1">210</td><td colspan="1" rowspan="1">2.4%</td><td colspan="1" rowspan="1">207</td><td colspan="1" rowspan="1">4.3%</td><td colspan="1" rowspan="1">-2.0%</td></tr><tr><td colspan="1" rowspan="1">Musculoskeletal</td><td colspan="1" rowspan="1">150</td><td colspan="1" rowspan="1">1.3%</td><td colspan="1" rowspan="1">150</td><td colspan="1" rowspan="1">2.0%</td><td colspan="1" rowspan="1">-0.7%</td></tr><tr><td colspan="1" rowspan="1">Neurological</td><td colspan="1" rowspan="1">207</td><td colspan="1" rowspan="1">4.3%</td><td colspan="1" rowspan="1">207</td><td colspan="1" rowspan="1">2.9%</td><td colspan="1" rowspan="1">1.4%</td></tr></table>

An additional analysis was performed on the 3881 pairs in which diagnoses were recorded using both reading modalities (MD and MO) by the same pathologist. The reported diagnoses from each case were directly compared between the two reading modalities (without comparison to SD) for each pathologist. Each diagnosis deemed “Major Discordance” to SD by the adjudication panel was compared to that same pathologist’s diagnosis using the other reading modality. If the diagnoses for both reading modalities were the same, that pair result was “agree”. Otherwise, the pair was marked “disagree”. All pairs not containing a “Major Discordance” were assumed to be “agree”. Using this methodology, all available pairs were scored and the summary of the agreement rate per site is listed below in Table 9 below.

Table 9: Agreement Rate Between MD and MO (without comparing to SD)   

<table><tr><td rowspan=2 colspan=1>Study sites</td><td rowspan=2 colspan=1>Number ofcases</td><td rowspan=2 colspan=1>Number ofreads</td><td rowspan=1 colspan=2>Agree</td><td rowspan=1 colspan=2>Disagree</td></tr><tr><td rowspan=1 colspan=1>N</td><td rowspan=1 colspan=1>%</td><td rowspan=1 colspan=1>N</td><td rowspan=1 colspan=1>%</td></tr><tr><td rowspan=1 colspan=1>Site 1</td><td rowspan=1 colspan=1>569</td><td rowspan=1 colspan=1>1706</td><td rowspan=1 colspan=1>1671</td><td rowspan=1 colspan=1>97.9%</td><td rowspan=1 colspan=1>35</td><td rowspan=1 colspan=1>2.1%</td></tr><tr><td rowspan=1 colspan=1>Site 2</td><td rowspan=1 colspan=1>429</td><td rowspan=1 colspan=1>1287</td><td rowspan=1 colspan=1>1243</td><td rowspan=1 colspan=1>96.6%</td><td rowspan=1 colspan=1>44</td><td rowspan=1 colspan=1>3.4%</td></tr><tr><td rowspan=1 colspan=1>Site 3</td><td rowspan=1 colspan=1>296</td><td rowspan=1 colspan=1>888</td><td rowspan=1 colspan=1>876</td><td rowspan=1 colspan=1>98.6%</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>1.4%</td></tr><tr><td rowspan=1 colspan=1>Total</td><td rowspan=1 colspan=1>1294</td><td rowspan=1 colspan=1>3881</td><td rowspan=1 colspan=1>3790</td><td rowspan=1 colspan=1>97.7%</td><td rowspan=1 colspan=1>91</td><td rowspan=1 colspan=1>2.3%</td></tr></table>

# D. Human Factor Study:

Human factors studies for $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution to assess performance of critical user tasks and use scenarios by representative users, including anatomic pathology lab technicians and pathologists were conducted. The studies were designed around critical user tasks and use scenarios performed by representative users from histotechnicians and pathologists (15 pathologists and 15 laboratory technicians) to assess software interface, hardware interface, and product labeling. All participants were able to perform all tasks (including the critical tasks) and no critical task failures were observed. Critical tasks were identified internally by conducting a detailed uFMEA (user Failure Modes and Effects Analysis). Both user groups perceived that the risk mitigation steps for critical tasks were successfully implemented and that the $\operatorname { E } 1 0 0 0 \operatorname { D } \mathbf { x }$ Digital Pathology Solution can be used safely and effectively by its intended users, for its intended purpose, and in its intended use environment.

# VIII Proposed Labeling:

The labeling is sufficient, and it satisfies the requirements of 21 CFR Parts 801 and 809, as applicable, and the special controls for this device type under 21 CFR 864.3700.

# IX Conclusion:

The submitted information in this premarket notification is complete and supports a substantial equivalence decision.